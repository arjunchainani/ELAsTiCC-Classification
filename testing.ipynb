{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe16fc0c-372c-4432-9b95-82a1b103b97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 12:03:13.336467: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# !pip install --user numpy polars tqdm astropy tf_keras tensorflow dataframe_image==0.2.5\n",
    "# uncomment line above if necessary\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from collections import OrderedDict\n",
    "from taxonomy import get_classification_labels, get_astrophysical_class, plot_colored_tree\n",
    "# import tf_keras as keras\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c7641c-d001-4707-8426-9abec2fc731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSST_Source:\n",
    "\n",
    "    # List of time series features actually stored in the instance of the class.\n",
    "    time_series_features = ['MJD', 'BAND', 'PHOTFLAG', 'FLUXCAL', 'FLUXCALERR']\n",
    "\n",
    "    # List of other features actually stored in the instance of the class.\n",
    "    other_features = ['RA', 'DEC', 'MWEBV', 'MWEBV_ERR', 'REDSHIFT_HELIO', 'REDSHIFT_HELIO_ERR', 'VPEC', 'VPEC_ERR', 'HOSTGAL_PHOTOZ', 'HOSTGAL_PHOTOZ_ERR', 'HOSTGAL_SPECZ', 'HOSTGAL_SPECZ_ERR', 'HOSTGAL_RA', 'HOSTGAL_DEC', 'HOSTGAL_SNSEP', 'HOSTGAL_DDLR', 'HOSTGAL_LOGMASS', 'HOSTGAL_LOGMASS_ERR', 'HOSTGAL_LOGSFR', 'HOSTGAL_LOGSFR_ERR', 'HOSTGAL_LOGsSFR', 'HOSTGAL_LOGsSFR_ERR', 'HOSTGAL_COLOR', 'HOSTGAL_COLOR_ERR', 'HOSTGAL_ELLIPTICITY', 'HOSTGAL_MAG_u', 'HOSTGAL_MAG_g', 'HOSTGAL_MAG_r', 'HOSTGAL_MAG_i', 'HOSTGAL_MAG_z', 'HOSTGAL_MAG_Y', 'HOSTGAL_MAGERR_u', 'HOSTGAL_MAGERR_g', 'HOSTGAL_MAGERR_r', 'HOSTGAL_MAGERR_i', 'HOSTGAL_MAGERR_z', 'HOSTGAL_MAGERR_Y']\n",
    "\n",
    "    # Additional features computed based on time_series_features and other_features mentioned in SNANA fits.\n",
    "    custom_engineered_features = ['MW_plane_flag', 'ELAIS_S1_flag', 'XMM-LSS_flag', 'Extended_Chandra_Deep_Field-South_flag', 'COSMOS_flag']\n",
    "\n",
    "    # Get the mean wavelengths for each filter and then convert to micro meters\n",
    "    pb_wavelengths = {\n",
    "        'u': (320 + 400) / (2 * 1000),\n",
    "        'g': (400 + 552) / (2 * 1000),\n",
    "        'r': (552 + 691) / (2 * 1000),\n",
    "        'i': (691 + 818) / (2 * 1000),\n",
    "        'z': (818 + 922) / (2 * 1000),\n",
    "        'Y': (950 + 1080) / (2 * 1000),\n",
    "    }\n",
    "\n",
    "    # Pass band to color dict\n",
    "    colors = OrderedDict({\n",
    "        'u': 'blue',\n",
    "        'g': 'green',\n",
    "        'r': 'red',\n",
    "        'i': 'teal',\n",
    "        'z': 'orange',\n",
    "        'Y': 'purple',\n",
    "    })\n",
    "\n",
    "    # 6 broadband filters used in LSST.\n",
    "    LSST_bands = list(colors.keys())\n",
    "\n",
    "    # Coordinates for LSST's 4 selected deep drilling fields. (Reference: https://www.lsst.org/scientists/survey-design/ddf)\n",
    "    LSST_DDF = {\n",
    "        'ELAIS_S1': SkyCoord(l=311.30 * u.deg, b=-72.90 * u.deg, frame='galactic'),\n",
    "        'XMM-LSS': SkyCoord(l=171.20 * u.deg, b=-58.77 * u.deg, frame='galactic'),\n",
    "        'Extended_Chandra_Deep_Field-South': SkyCoord(l=224.07 * u.deg, b=-54.47 * u.deg, frame='galactic'),\n",
    "        'COSMOS': SkyCoord(l=236.83 * u.deg, b=42.09 * u.deg, frame='galactic'),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Threshold values\n",
    "\n",
    "    # MW_plane_flag is set to one if |self.b| <= b_threshold. Indicative of weather the object is in the galactic plane.\n",
    "    b_threshold = 15 \n",
    "\n",
    "    # Flux scaling value\n",
    "    flux_scaling_const = 1000\n",
    "\n",
    "    # Radius of the deep drilling field for LSST, in degrees.\n",
    "    ddf_separation_radius_threshold = 3.5 / 2\n",
    "\n",
    "\n",
    "    def __init__(self, parquet_row) -> None:\n",
    "        \"\"\"Create an LSST_Source object to store both photometric and host galaxy data from the Elasticc simulations.\n",
    "\n",
    "        Args:\n",
    "            parquet_row (_type_): A row from the polars data frame that was generated from the Elasticc FITS files using fits_to_parquet.py\n",
    "            class_label (str): The Elasticc class label for this LSST_Source object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set all the class attributes\n",
    "        setattr(self, 'ELASTICC_class', parquet_row['ELASTICC_class'].to_numpy()[0])\n",
    "        setattr(self, 'SNID', parquet_row['SNID'].to_numpy()[0])\n",
    "        setattr(self, 'astrophysical_class', get_astrophysical_class(self.ELASTICC_class))\n",
    "\n",
    "        for key in parquet_row.columns:\n",
    "            if key in self.other_features:\n",
    "                setattr(self, key, parquet_row[key].to_numpy()[0])\n",
    "            elif key in self.time_series_features:\n",
    "                setattr(self, key, parquet_row[key][0].to_numpy())\n",
    "\n",
    "        # Run processing code on the light curves\n",
    "        self.process_lightcurve()\n",
    "\n",
    "        # Computer additional features\n",
    "        self.compute_custom_features()\n",
    "\n",
    "    \n",
    "    def process_lightcurve(self) -> None:\n",
    "        \"\"\"Process the flux information with phot flags. Processing is done using the following steps:\n",
    "        1. Remove saturations.\n",
    "        Finally, all the time series data is modified to conform to the steps mentioned above.\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove saturations from the light curves\n",
    "        saturation_mask =  (self.PHOTFLAG & 1024) == 0 \n",
    "\n",
    "        # Alter time series data to remove saturations\n",
    "        for time_series_feature in self.time_series_features:\n",
    "            setattr(self, time_series_feature, getattr(self, time_series_feature)[saturation_mask])\n",
    "        \n",
    "    def compute_custom_features(self) -> None:\n",
    "\n",
    "        source_coord = SkyCoord(ra = self.RA * u.deg, dec=self.DEC * u.deg)\n",
    "\n",
    "        # Check if the object is close to the galactic plane of the milky way\n",
    "        if abs(source_coord.galactic.b.degree) < self.b_threshold: \n",
    "            self.MW_plane_flag = 1\n",
    "        else:\n",
    "            self.MW_plane_flag = 0\n",
    "        \n",
    "        # Check if the object is in one of 4 LSST DDF's and set flags appropriately\n",
    "        for key in self.LSST_DDF:\n",
    "\n",
    "            # Separation from field center\n",
    "            separation = source_coord.separation(self.LSST_DDF[key]).degree\n",
    "\n",
    "            if separation < self.ddf_separation_radius_threshold:\n",
    "                setattr(self, f'{key}_flag', 1)\n",
    "            else:\n",
    "                setattr(self, f'{key}_flag', 0)\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def plot_flux_curve(self) -> None:\n",
    "        \"\"\"Plot the SNANA calibrated flux vs time plot for all the data in the processed time series. All detections are marked with a star while non detections are marked with dots. Observations are color codded by their passband. This function is fundamentally a visualization tool and is not intended for making plots for papers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Colorize the data\n",
    "        c = [self.colors[band] for band in self.BAND]\n",
    "        patches = [mpatches.Patch(color=self.colors[band], label=band, linewidth=1) for band in self.colors]\n",
    "        fmts = np.where((self.PHOTFLAG & 4096) != 0, '*', '.')\n",
    "\n",
    "        # Plot flux time series\n",
    "        for i in range(len(self.MJD)):\n",
    "            plt.errorbar(x=self.MJD[i], y=self.FLUXCAL[i], yerr=self.FLUXCALERR[i], color=c[i], fmt=fmts[i], markersize = '10')\n",
    "\n",
    "        # Labels\n",
    "        plt.title(f\"SNID: {self.SNID} | CLASS: {self.ELASTICC_class}\")\n",
    "        plt.xlabel('Time (MJD)')\n",
    "        plt.ylabel('Calibrated Flux')\n",
    "        plt.legend(handles=patches)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def get_classification_labels(self):\n",
    "        \"\"\"Get the classification labels (hierarchical) for this LSST Source object in the Taxonomy tree.\n",
    "\n",
    "        Returns:\n",
    "            (tree_nodes, numerical_labels): A tuple containing two list like objects. The first object contains the ordering of the nodes. The second list contains the labels themselves (0 when the object does not belong to the class and 1 when it does). The labels in the second object correspond to the nodes in the first object.\n",
    "        \"\"\"\n",
    "        return get_classification_labels(self.astrophysical_class)\n",
    "    \n",
    "    def plot_classification_tree(self):\n",
    "        \"\"\"Plot the classification tree (based on our taxonomy) for this LSST Source object.\n",
    "        \"\"\"\n",
    "\n",
    "        node, labels = self.get_classification_labels()\n",
    "        plot_colored_tree(labels)\n",
    "\n",
    "\n",
    "    def get_event_table(self):\n",
    "\n",
    "        # Dataframe for time series data\n",
    "        table = Table()\n",
    "\n",
    "        # Find time since last observation\n",
    "        time_since_first_obs = self.MJD - self.MJD[0]\n",
    "        table['scaled_time_since_first_obs'] = time_since_first_obs / 100\n",
    "\n",
    "        # 1 if it was a detection, zero otherwise\n",
    "        table['detection_flag'] = np.where((self.PHOTFLAG & 4096 != 0), 1, 0)\n",
    "\n",
    "        # Transform flux cal and flux cal err to more manageable values (more consistent order of magnitude)\n",
    "        table['scaled_FLUXCAL'] = self.FLUXCAL / self.flux_scaling_const\n",
    "        table['scaled_FLUXCALERR'] = self.FLUXCALERR / self.flux_scaling_const\n",
    "\n",
    "        # One hot encoding for the pass band\n",
    "        table['band_label'] = [self.pb_wavelengths[pb] for pb in self.BAND]\n",
    "\n",
    "        # Consistency check\n",
    "        assert len(table) == len(self.MJD), \"Length of time series tensor does not match the number of mjd values.\"\n",
    "\n",
    "        # Array for static features\n",
    "        feature_static = OrderedDict()\n",
    "        for other_feature in self.other_features:\n",
    "            feature_static[other_feature] = getattr(self, other_feature)\n",
    "\n",
    "        for feature in self.custom_engineered_features:\n",
    "            feature_static[feature] = getattr(self, feature)\n",
    "\n",
    "        # Array for computed static features\n",
    "        table.meta = feature_static\n",
    "        return table\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "\n",
    "        to_return = str(vars(self))\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebcc5a2-e52b-4aa6-b900-33a690c5ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSSTSourceDataSet():\n",
    "\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            path (string): Parquet file.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Loading parquet dataset: {path}', flush=True)\n",
    "\n",
    "        self.path = path\n",
    "        self.parquet = pl.read_parquet(path)\n",
    "        self.num_sample = self.parquet.shape[0]\n",
    "\n",
    "        print(f\"Number of sources: {self.num_sample}\")\n",
    "\n",
    "    def get_len(self):\n",
    "\n",
    "        return self.num_sample\n",
    "\n",
    "    def get_item(self, idx):\n",
    "        \n",
    "        row = self.parquet[idx]\n",
    "        source = LSST_Source(row)\n",
    "        table = source.get_event_table()\n",
    "\n",
    "        astrophysical_class = get_astrophysical_class(source.ELASTICC_class)\n",
    "        _, class_labels = get_classification_labels(astrophysical_class)\n",
    "        class_labels = np.array(class_labels)\n",
    "        snid = source.SNID\n",
    "\n",
    "        return source, class_labels, snid\n",
    "\n",
    "    def get_item_from_snid(self, snid):\n",
    "        row = self.parquet.filter(pl.col('SNID') == snid)\n",
    "        # row = self.pandas.loc[self.pandas['SNID'] == snid]\n",
    "        # print(row)\n",
    "        source = LSST_Source(row)\n",
    "        table = source.get_event_table()\n",
    "\n",
    "        astrophysical_class = get_astrophysical_class(source.ELASTICC_class)\n",
    "        _, class_labels = get_classification_labels(astrophysical_class)\n",
    "        class_labels = np.array(class_labels)\n",
    "        snid = source.SNID\n",
    "\n",
    "        return source, class_labels, snid\n",
    "    \n",
    "    def get_dimensions(self):\n",
    "\n",
    "        idx = 0\n",
    "        source, class_labels = self.get_item(idx)\n",
    "        table = source.get_event_table()\n",
    "\n",
    "        ts_np = table.to_pandas().to_numpy()\n",
    "        static_np = np.array(list(table.meta.values()))\n",
    "\n",
    "        dims = {\n",
    "            'ts': ts_np.shape[1],\n",
    "            'static': static_np.shape[0],\n",
    "            'labels': class_labels.shape[0]\n",
    "        }\n",
    "\n",
    "        return dims\n",
    "    \n",
    "    def get_labels(self):\n",
    "\n",
    "        ELASTICC_labels = self.parquet['ELASTICC_class']\n",
    "        astrophysical_labels = []\n",
    "\n",
    "        for idx in range(self.num_sample):\n",
    "\n",
    "            elasticc_class = ELASTICC_labels[idx]\n",
    "            astrophysical_class = get_astrophysical_class(elasticc_class)\n",
    "            astrophysical_labels.append(astrophysical_class)\n",
    "        \n",
    "        return astrophysical_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720a7493-bfe1-4edc-843c-ecbe23d0ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.9G\n",
      "drwxrwx---  4 arjun15 arjun15  512 Mar 25 11:26 .\n",
      "drwx------ 19 arjun15 arjun15 4.0K Apr  8 11:57 ..\n",
      "-rw-rw----  1 arjun15 arjun15 2.1M Mar 25 11:26 a_labels.pkl\n",
      "drwxrwx--- 14 arjun15 arjun15 4.0K Feb 16 14:57 augmented\n",
      "-rw-rw----  1 arjun15 arjun15  14M Mar 25 11:26 e_label.pkl\n",
      "drwxrwx---  2 arjun15 arjun15  512 Feb 14 22:16 .ipynb_checkpoints\n",
      "-rw-rw----  1 arjun15 arjun15 2.3M Mar 25 11:26 lengths.pkl\n",
      "-rw-rw----  1 arjun15 arjun15 1.8K Mar 20 09:41 phase.pkl\n",
      "-rw-rw----  1 arjun15 arjun15 308M Mar 25 11:28 x_static.pkl\n",
      "-rw-rw----  1 arjun15 arjun15 5.3G Mar 25 12:03 x_ts.pkl\n",
      "-rw-rw----  1 arjun15 arjun15 246M Mar 25 11:28 y.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -alh pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbca5db-87ba-4f29-b720-c6060b890c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5285427-944d-4f64-8d23-b1520d1bfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxonomy import get_taxonomy_tree\n",
    "class_count = 10\n",
    "ts_length = 500\n",
    "ts_flag_value = 0\n",
    "\n",
    "model = keras.models.load_model(f\"models/lsst_alpha_0.5/best_model.h5\", compile=False)\n",
    "\n",
    "tree = get_taxonomy_tree()\n",
    "X_ts = load(\"pickles/x_ts.pkl\")\n",
    "X_static = load(\"pickles/x_static.pkl\")\n",
    "Y = load(\"pickles/y.pkl\")\n",
    "astrophysical_classes = load(\"pickles/a_labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede844af-febc-4dd5-8560-cbdc61a1a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title=None, img_file=None):\n",
    "    \n",
    "    n_class = len(labels)\n",
    "    font = {'size'   : 25}\n",
    "    plt.rc('font', **font)\n",
    "    \n",
    "    cm = np.round(confusion_matrix(y_true, y_pred, labels=labels, normalize='true'),2)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    disp.im_.colorbar.remove()\n",
    "    \n",
    "    fig = disp.figure_\n",
    "    if n_class > 10:\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=45)\n",
    "    \n",
    "    fig.set_figwidth(18)\n",
    "    fig.set_figheight(18)\n",
    "    \n",
    "    for label in disp.text_.ravel():\n",
    "        if n_class > 10:\n",
    "            label.set_fontsize(12)\n",
    "        elif n_class <= 10 and n_class > 3:\n",
    "            disp.ax_.tick_params(axis='both', labelsize=40)\n",
    "            label.set_fontsize('xx-large')\n",
    "        else:\n",
    "            disp.ax_.tick_params(axis='both', labelsize=40)\n",
    "            label.set_fontsize('xx-large')\n",
    "    \n",
    "    if title:\n",
    "        disp.ax_.set_xlabel(\"Predicted Label\", fontsize=60)\n",
    "        disp.ax_.set_ylabel(\"True Label\", fontsize=60)\n",
    "        disp.ax_.set_title(title, fontsize=45, wrap=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    if img_file:\n",
    "        plt.savefig(img_file)\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49d681-1658-46c6-a153-66cde7ce3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix_with_peaks(y_true, y_pred, phase, day, labels, title=None, img_file=None):\n",
    "    n_class = len(labels)\n",
    "    font = {'size'   : 35}\n",
    "    plt.rc('font', **font)\n",
    "    \n",
    "    cm = np.round(confusion_matrix(y_true, y_pred, labels=labels, normalize='true'),2)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    disp.im_.colorbar.remove()\n",
    "    \n",
    "    fig = disp.figure_\n",
    "    if n_class > 10:\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=45)\n",
    "    \n",
    "    fig.set_figwidth(18)\n",
    "    fig.set_figheight(18)\n",
    "    \n",
    "    for label in disp.text_.ravel():\n",
    "        if n_class > 10:\n",
    "            label.set_fontsize(12)\n",
    "        elif n_class <= 10 and n_class > 3:\n",
    "            disp.ax_.tick_params(axis='both', labelsize=40)\n",
    "            label.set_fontsize('xx-large')\n",
    "        else:\n",
    "            disp.ax_.tick_params(axis='both', labelsize=40)\n",
    "            label.set_fontsize('xx-large')\n",
    "    \n",
    "    if title:\n",
    "        disp.ax_.set_xlabel(\"Predicted Label\", fontsize=60)\n",
    "        disp.ax_.set_ylabel(\"True Label\", fontsize=60)\n",
    "        disp.ax_.set_title(title, fontsize=45, wrap=True)\n",
    "\n",
    "    handles = [plt.Line2D([0], [0], color='white', marker='o', linestyle='', label=f'{np.unique(astrophysical_classes)[i]} = {phase[day, i]}') for i in range(19)]\n",
    "    plt.legend(handles=handles, title='Days Since Peak MJD', loc='upper left', bbox_to_anchor=(1.05, 1), fontsize='xx-small')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if img_file:\n",
    "        plt.savefig(img_file)\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adeb96-81a3-4e3b-b0b8-53e592820da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxonomy import source_node_label, get_taxonomy_tree\n",
    "from interpret_results import save_leaf_cf_and_rocs\n",
    "\n",
    "def determine_anomalies(class_probs, tree, purity_threshold=0.7):\n",
    "    level_order_nodes = list(nx.bfs_tree(tree, source=source_node_label).nodes())\n",
    "    leaf_nodes = level_order_nodes[-19:]\n",
    "    \n",
    "    indiv_probs = [np.squeeze(class_probs[index][0]) for index in range(len(class_probs))]\n",
    "    true_classes = [class_probs[i][1] for i in range(len(class_probs))]\n",
    "\n",
    "    true_class_ids = [np.argmax(true_classes[index][-19:]) for index in range(len(true_classes))]\n",
    "    true_class_names = []\n",
    "    for class_idx in tqdm(true_class_ids, desc='Calculating Labels: '):\n",
    "        true_class_names.append(leaf_nodes[class_idx])\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    class_df = pd.DataFrame(true_class_names, columns=['True Class:'])\n",
    "    class_probs_df = pd.DataFrame(indiv_probs)\n",
    "    df = pd.concat([class_df, class_probs_df], axis=1, ignore_index=True)\n",
    "    columns = level_order_nodes.copy()\n",
    "    columns.insert(0, 'True Class:')\n",
    "    df.columns = columns\n",
    "    display(df)\n",
    "\n",
    "    results = df\n",
    "    anomaly_labels = results[['KN', 'uLens', 'SLSN', 'PISN', 'TDE', 'CART', 'ILOT']]\n",
    "    non_anomaly_labels = results[['Cepheid', 'RR Lyrae', 'Delta Scuti', 'EB', 'SNIa', 'SNIax', 'SNIb/c', 'SNI91bg', 'SNII', 'M-dwarf Flare', 'Dwarf Novae', 'AGN']]\n",
    "    anomaly_preds = [np.sum(lightcurve[1][::]) for lightcurve in anomaly_labels.iterrows()]\n",
    "    non_anomaly_preds = [np.sum(lightcurve[1][::]) for lightcurve in non_anomaly_labels.iterrows()]\n",
    "    anomaly_detections = list(enumerate(zip(anomaly_preds, non_anomaly_preds)))\n",
    "\n",
    "    leaf_labels = np.array(level_order_nodes)[-19:]\n",
    "    pred_labels = [leaf_labels[i] for i in np.argmax(np.array(indiv_probs)[:, -19:], axis=1)]\n",
    "\n",
    "    anomaly = pd.DataFrame({'Anomaly Probability': [anomaly_detections[row][1][0] for row in range(len(anomaly_detections))]})\n",
    "    not_anomaly = pd.DataFrame({'Not Anomaly Probability': [anomaly_detections[row][1][1] for row in range(len(anomaly_detections))]})\n",
    "    \n",
    "    anomaly.reset_index(drop=True, inplace=True)\n",
    "    not_anomaly.reset_index(drop=True, inplace=True)\n",
    "    preds_df = pd.concat([class_df, anomaly, not_anomaly.set_axis(anomaly.index)], axis=1)\n",
    "    display(preds_df)\n",
    "\n",
    "    labels = [class_probs[i][1][-19:] for i in range(len(class_probs))]\n",
    "    anomaly_map = [\n",
    "        0.0, # AGN - not anomaly\n",
    "        0.0, # SNIa - not anomaly\n",
    "        0.0, # SNIb/c - not anomaly\n",
    "        0.0, # SNIax - not anomaly\n",
    "        1.0, # SNI91bg - anomaly\n",
    "        0.0, # SNII - not anomaly\n",
    "        1.0, # KN - anomaly\n",
    "        0.0, # Dwarf Novae - not anomaly\n",
    "        1.0, # uLens - anomaly\n",
    "        0.0, # M-dwarf Flare - not anomaly\n",
    "        1.0, # SLSN - anomaly\n",
    "        1.0, # TDE - anomaly\n",
    "        1.0, # ILOT - anomaly\n",
    "        1.0, # CART - anomaly\n",
    "        1.0, # PISN - anomaly\n",
    "        0.0, # Cepheid - not anomaly\n",
    "        0.0, # RR Lyrae - not anomaly\n",
    "        0.0, # Delta Scuti - not anomaly\n",
    "        0.0  # EB - not anomaly\n",
    "    ]\n",
    "    \n",
    "    y_true = [[anomaly_map[np.argmax(label)], 1.0 - anomaly_map[np.argmax(label)]] for label in labels]\n",
    "    anomaly = np.array([anomaly_detections[row][1][0] for row in range(len(anomaly_detections))])\n",
    "    non_anomaly = np.array([anomaly_detections[row][1][1] for row in range(len(anomaly_detections))])\n",
    "    y_pred = np.stack((anomaly, non_anomaly), axis=1)\n",
    "    print(y_true)\n",
    "\n",
    "    true = [np.argmax(y_true[i]) for i in range(len(y_true))]\n",
    "    pred = [0 if element > purity_threshold else 1 for element in np.transpose(y_pred)[0]]\n",
    "\n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee1f75-24ac-4d8e-808f-729ceee116a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_ts_length_to_days_since_trigger(X_ts, X_static, Y, a_classes, days):\n",
    "\n",
    "    # Augment the length of the ts data\n",
    "    X_ts = get_ts_upto_days_since_trigger(X_ts, days=days)\n",
    "\n",
    "    # Squeeze data into homogeneously shaped numpy arrays\n",
    "    X_ts = np.squeeze(X_ts)\n",
    "    X_static = np.squeeze(X_static)\n",
    "    Y = np.squeeze(Y).astype(np.float32)\n",
    "    astrophysical_classes = np.squeeze(a_classes)\n",
    "\n",
    "    return X_ts, X_static, Y, astrophysical_classes\n",
    "\n",
    "def get_ts_upto_days_since_trigger(X_ts, days, add_padding=True):\n",
    "\n",
    "    augmented_list = []\n",
    "\n",
    "    # Loop through all the data\n",
    "    for ind in tqdm(range(len(X_ts)), desc =\"TS Augmentation: \"):\n",
    "\n",
    "        times = X_ts[ind]['scaled_time_since_first_obs'].to_numpy()\n",
    "\n",
    "        # Get the idx of the first detection\n",
    "        first_detection_idx = np.where(X_ts[ind]['detection_flag'].to_numpy() == 1)[0][0]\n",
    "        first_detection_t = times[first_detection_idx]\n",
    "\n",
    "        if len(np.where((times - first_detection_t) * 100 <= days)[0]) == 0:\n",
    "            augmented_list.append(np.zeros_like(X_ts[ind].to_numpy()))\n",
    "        else:\n",
    "            # Get the index of the the last observation between the mjd(first detection) and  mjd(first detection)\n",
    "            last_observation_idx = np.where((times - first_detection_t) * 100 <= days)[0][-1]\n",
    "            \n",
    "            # Slice the data appropriately, Keep the first new_length number of observations and all columns\n",
    "            augmented_list.append(X_ts[ind].to_numpy()[:(last_observation_idx + 1), :])\n",
    "\n",
    "    # Optionally - Pad for TF masking layer\n",
    "    if add_padding:\n",
    "        augmented_list = pad_sequences(augmented_list, maxlen=ts_length,  dtype='float32', padding='post', value=ts_flag_value)\n",
    "\n",
    "    return augmented_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f573da3-4383-49af-a688-a71cc23dcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import augment_ts_length_to_days_since_trigger, get_ts_upto_days_since_trigger, get_augmented_data\n",
    "from interpret_results import save_all_cf_and_rocs, save_leaf_cf_and_rocs, get_conditional_probabilites\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "days = 2 ** np.array(range(11))\n",
    "default_batch_size = 1024\n",
    "\n",
    "def save(save_path , obj):\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def run_day_wise_anomaly_detection_analysis(model, tree, model_dir, X_ts, X_static, Y, astrophysical_classes, purity_threshold=0.7, path=\"plots/daywise/\"):\n",
    "    all_predictions = []\n",
    "    all_trues = []\n",
    "\n",
    "    precision_values = []\n",
    "\n",
    "    for i, d in enumerate(days):\n",
    "        print(f'Running inference for trigger + {d} days...')\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        # print('Augmenting Data...')\n",
    "        # x1, x2, y_true, _ = augment_ts_length_to_days_since_trigger(X_ts, X_static, Y, astrophysical_classes, d)\n",
    "        # save(f\"pickles/augmented/day{d}/y.pkl\", y_true)\n",
    "\n",
    "        # x1 = load(f\"pickles/augmented/day{d}/x1.pkl\")[:2]\n",
    "        # x2 = load(f\"pickles/augmented/day{d}/x2.pkl\")[:2]\n",
    "        # y_true = load(f\"pickles/augmented/day{d}/y.pkl\")\n",
    "        # print('Loaded!')\n",
    "\n",
    "        if not os.path.exists(f'pickles/augmented/day{d}/pred.pkl'):\n",
    "            print('Passing through model...')\n",
    "            y_pred = model.predict([x1, x2], batch_size=default_batch_size)\n",
    "            cm = save_leaf_cf_and_rocs(y_true, y_pred, tree, path, \"test\")\n",
    "            save(f\"pickles/augmented/day{d}/pred.pkl\", y_pred)\n",
    "            print('Saved to disk!')\n",
    "        else:\n",
    "            print(f'Loading y_pred values from disk for day {d}')\n",
    "            y_pred = load(f'pickles/augmented/day{d}/pred.pkl')\n",
    "            y_true = load(f'pickles/augmented/day{d}/y.pkl')\n",
    "            cm = save_leaf_cf_and_rocs(y_true, y_pred, tree, path, \"test\")\n",
    "            print('Loaded!')\n",
    "\n",
    "        _, pseudo_conditional_probabilities = get_conditional_probabilites(y_pred, tree)\n",
    "        probs_with_labels = list(zip(pseudo_conditional_probabilities, y_true))\n",
    "\n",
    "        print('Determining anomalies...')\n",
    "        true, pred = determine_anomalies(probs_with_labels, tree, purity_threshold)\n",
    "\n",
    "        print(f'For trigger + {d} days, these are the statistics:')\n",
    "\n",
    "        plot_title = f\"Trigger + {d} days, {purity_threshold * 100}% confidence threshold\"\n",
    "        \n",
    "        # Print all the stats and make plots...\n",
    "        labels = ['Anomaly', 'Not Anomaly']\n",
    "        for index, (label, prediction) in tqdm(enumerate(zip(true, pred))):\n",
    "            true[index] = labels[label]\n",
    "            pred[index] = labels[prediction]\n",
    "\n",
    "        current_cm = plot_confusion_matrix(true, pred, labels, title=plot_title, img_file=os.path.join(path, f'AD_{d}.png'))\n",
    "        # current_cm = plot_confusion_matrix_with_peaks(true, pred, phase, i, labels, title=plot_title, img_file=os.path.join(path, f'AD_{d}.png'))\n",
    "\n",
    "        # calculates the precision at the specific day and adds to data set\n",
    "        precision_values.append(current_cm[0][0] / (current_cm[0][0] + current_cm[1][0]))\n",
    "        \n",
    "    return precision_values\n",
    "\n",
    "def make_gif(files, gif_file=None):\n",
    "\n",
    "    # Load the images\n",
    "    images = []\n",
    "    for filename in files:\n",
    "        images.append(imageio.imread(filename))\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(18, 18))\n",
    "\n",
    "    # Create the animation\n",
    "    def animate(i):\n",
    "        ax.clear()\n",
    "        ax.axis('off')\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        ax.imshow(images[i])\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(images), interval=500)\n",
    "\n",
    "    if gif_file:\n",
    "        # Save the animation as a GIF\n",
    "        anim.save(gif_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230ac9b-35e2-4729-9f0b-0efa7c9fb937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for trigger + 1 days...\n",
      "Loading y_pred values from disk for day 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/lsst/gitlab/td_env-dev/2024-03-08-06-23/conda/envs/lsst-scipipe-4.1.0-exact/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/global/common/software/lsst/gitlab/td_env-dev/2024-03-08-06-23/conda/envs/lsst-scipipe-4.1.0-exact/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/global/common/software/lsst/gitlab/td_env-dev/2024-03-08-06-23/conda/envs/lsst-scipipe-4.1.0-exact/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "          AGN       0.43      0.37      0.40     76258\n",
      "         CART       0.00      0.00      0.00      8207\n",
      "      Cepheid       0.00      0.00      0.00     13771\n",
      "  Delta Scuti       0.00      0.00      0.00     20650\n",
      "  Dwarf Novae       0.00      0.00      0.00      8025\n",
      "           EB       0.88      0.00      0.00     66454\n",
      "         ILOT       0.00      0.00      0.00      7461\n",
      "           KN       0.01      1.00      0.01      4426\n",
      "M-dwarf Flare       0.00      0.00      0.00      1859\n",
      "         PISN       0.00      0.00      0.00     63586\n",
      "     RR Lyrae       0.00      0.00      0.00     14033\n",
      "         SLSN       0.02      0.03      0.03     66088\n",
      "      SNI91bg       0.00      0.00      0.00     28637\n",
      "         SNII       0.00      0.00      0.00    301544\n",
      "         SNIa       0.00      0.00      0.00    120739\n",
      "        SNIax       0.00      0.00      0.00     28030\n",
      "       SNIb/c       0.00      0.00      0.00    168254\n",
      "          TDE       0.07      0.00      0.00     66000\n",
      "        uLens       0.08      0.03      0.04     17592\n",
      "\n",
      "     accuracy                           0.03   1081614\n",
      "    macro avg       0.08      0.08      0.02   1081614\n",
      " weighted avg       0.09      0.03      0.03   1081614\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/lsst/gitlab/td_env-dev/2024-03-08-06-23/conda/envs/lsst-scipipe-4.1.0-exact/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/global/common/software/lsst/gitlab/td_env-dev/2024-03-08-06-23/conda/envs/lsst-scipipe-4.1.0-exact/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/global/common/software/lsst/gitlab/td_env-dev/2024-03-08-06-23/conda/envs/lsst-scipipe-4.1.0-exact/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "Loaded!\n",
      "Determining anomalies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Labels: 100%|██████████| 1081614/1081614 [00:00<00:00, 4911051.20it/s]\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(f\"models/lsst_alpha_0.5/best_model.h5\", compile=False)\n",
    "tree = get_taxonomy_tree()\n",
    "\n",
    "precision = run_day_wise_anomaly_detection_analysis(model, tree, None, X_ts, X_static, Y, astrophysical_classes, purity_threshold=0.7, path='plots/daywise/testing/')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf074e1-2caf-46f7-b3d1-8e43edca178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGN', 'SNIa', 'SNIb/c', 'SNIax', 'SNI91bg', 'SNII', 'KN', 'Dwarf Novae', 'uLens', 'M-dwarf Flare', 'SLSN', 'TDE', 'ILOT', 'CART', 'PISN', 'Cepheid', 'RR Lyrae', 'Delta Scuti', 'EB']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from taxonomy import get_taxonomy_tree, source_node_label\n",
    "\n",
    "tree = get_taxonomy_tree()\n",
    "\n",
    "level_order_nodes = list(nx.bfs_tree(tree, source=source_node_label).nodes())\n",
    "print(level_order_nodes[-19:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063e1dd-53e7-4417-8b77-30f73d64bd90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-td-env",
   "language": "python",
   "name": "desc-td-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
